\chapter{Introduction}
\label{chapter:intro}

\section{Motivation}
\label{sec:motivation}
Human beings have always felt the need to explore and find a way to a better life. Starting with stone tools (2.6 million years ago) and continuing with personal computers, one can clearly see widespread proofs of human evolution. In the latter years, humanity has discovered Machine Learning which has the purpose to allow machines learn to do different tasks.

Deep learning is a subfield of the machine learning area, drawing its inspiration from the human brain's functionality. Through deep learning, we are trying to solve some of the most pressuring human problems such as cancer classification (benign or malignant tumor)\cite{mitosis}, self-driving cars based on pedestrian detection\cite{pedestrian} or recognizing digits form photos taken with Google Street View\cite{svhn}, all using unsupervised learning of features.

This paper's proposes an in-depth look on solving reinforcement learning problems, using convolutional neural networks. More exactly, we want to design an agent which is capable of learning to play a game\cite{atari} seeing only the pixels from the frames and being rewarded after finishing the game.

This ideea was brought to attention in 2013, when Alex Graves et al. from DeepMind\footnote{\url{http://deepmind.com/}} came with the idea of creating a convolutional neural network capable of playing different Atari 2600 games, being almost as good as a game tester. We will see later what almost means. The motivation behind this experiment is not only connected with the fact that we have a machine capable of playing games, but the more important problem is that they achieved the generalization of a machine that could learn several types of games, which in fact is the starting of a new revolution in machine learning. If we are capable of making one unique algorithm that can solve multiple tasks, we are capable of simulating the real human brain and capable of reducing complex problems to others, much simpler thus we can reduce the programming burden and concentrate on solving the more urgent problems such as cancer classification.

Moreover, there are also other types of applications that can reveal the power of convolutional neural networks and one of them is the neural algorithm for creating paintings\cite{paintings} from ordinary pictures. The inputs is represented by two pictures, one which is the painting (\textbf{e.g} \textit{The Starry Night} of Vincent van Gogh) and the other one which is a normal photo, like a picture of a house. The output of the network is the house painted in van Gogh's technique.


\section{Project description}
\label{sec:proj-description}
This paper presents two different algorithms aiding the creation of a good agent which can learn to play games. It is worth mentioning that the agent does not know the rules and can not infere most of the rules at the beginning of the game. The whole topic has been split into three parts: running Q-Learning on Hanoi Towers game, learning values predicted by Q-Learning using a convolutional neural network and binding the first two parts together.

First of them is the classical Q-Learning, which searches for the optimal policy for taking actions in each state of the game. We will discus about the exploration vs exploitation problem, how to determine the learning rate suitable for our purposes, how many episodes we have to play until we determine a policy close to the optimal one.

The second topic we approach in this paper is the possibility of predicting a continuous output from an image. Practically, we take the dataset from Q-Learning which contains images as input and a table of four values corresponding to the four possible actions in the Hanoi Tower games: up, down, left and right.
Here, we can determine whether we have a good or a bad convolutional neural network. In other words, we will test the capacity of learning samples, the capacity for generalization.

One must pay attention to the data pre-processing, this being an important step in data selection. For example, when we learn to classify things, we may not need the color information. Converting data from RGB to YUV gives us the possibility of separating luminance from chrominance.

After that we need to look closer at the model. We need a model that can be proven to converge at the optimal solution and also a model that can learn very fast. Datasets are very big and can contain millions of pictures.

It is also important that one knows when to stop training to avoid overfitting, our primary target being that of minimizing the test error. We have to take care of splitting the dataset in training, testing or validation, how to determine when to stop training the network and how to formalize the results afterwards.

Another important step is called the loss function, the moment when we determine how far we are from our target. Here, we can find the border between underfitting and overfitting, called the optimum state. If we have high bias, it means that we have underfitting and our classifier is not able to predict data well and if we have high variance, it means that we have solid training samples for learning, but new samples cannot be predicted.

The last part is the one where we combine the first two modules. We will try to get a closer look on what those from DeepMind have done with the Atari paper\cite{atari}. The most important part is the connection between the Q-Learning algorithm and the convolutional neural network. We will see how weights are updated according to rewards received during the game.

The main problem is that of creating an architecture capable of playing any game without adapting initial meta parameters. All the above-mentioned problems will be discussed in this aper and formalized using experimental results, plots or anything that can serve as a proof.
\newpage

\section{Technologies}
\label{sec:technologies}
All algorithms described in this paper have been implemented using Torch7\footnote{\url{http://torch.ch/}}, a deep learning framework written in Lua\footnote{\url{http://www.lua.org/about.html}}, a scripting language based on a C API. For creating the game, generating frames or modifying images the LOVE platform\footnote{\url{https://love2d.org/wiki/Main_Page}} has been used, this also being written in Lua.  For interactive computing(image/filter visualization) iTorch\footnote{\url{https://github.com/facebook/iTorch}} is preferred. For illustrating different functions Octave\footnote{\url{https://www.gnu.org/software/octave/}} is the best open-source choice.

Why Lua and Torch? They provide a fast environment as opposed to others\cite{torch7}, multiple modules with functions already implemented such as transfer functions(tanh, sigmoid), loss functions(Mean Squared Error, Negative Log Likelihood) or convolutional layers(SpatialMaxPooling, SpatialSubSampling)

\section{Structure of this paper}
\label{sec:paper_structure}

\labelindexref{Chapter}{chapter:state} \textbf{State of the art} presents the entire architecture of a network based on a complex research in the machine learning, pattern recognition and reinforcement learning fields. Also, an in-depth look into the ``Human-level control through deep reinforcement learning'' paper is provided. This paper has been chosen for the fact that it is the primary resource which proves that the combination of deep learning with reinforcement learning is possible. 

\labelindexref{Chapter}{chapter:system-design} \textbf{System design and implementation} describes several architectures through which the combination between deep learning and reinforcement learning is possible. Every step in choosing the architecture is well documented, either with mathematical reasoning or with empirical reasoning.

\labelindexref{Chapter}{chapter:conclusions} \textbf{Conclusions} is dedicated to the personal views of the paper's author regarding this topic and also presents the systematic investigation in gathering information.

\labelindexref{Chapter}{chapter:future-work} \textbf{Future work} describes future tunings of the architectures and also further investigation on one of the following fields: machine learning, pattern recognition or reinforcement learning.

\section{Copyright infringement}
\label{sec:copyright}
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. 







