
\chapter{State of the art}
\label{chapter:state}

Being a large domain, deep learning imposes a research into multiple subdomains such as neuroscience, reinforcement learning, neural networks or convolutional neural networks. That being said, this chapter is dedicated to gathering information on each of them.

\section{Reinforcement learning}
\label{sec:reinforcement}

\subsection{History}
The history of reinforcement learning starts with the pavlovian experiment\cite{pavlov} in 1903, when Pavlov tried to demonstrate that there is a connection between conditioned and unconditioned stimulus. The unconditioned response of salivation of a dog is fired up by bringing an unconditioned stimulus such as food. If we try to make the dog salivate in the presence of a neutral stimulus(the sound of a bell) we won't succeed, but if we use at the same time the unconditioned and neutral stimulus we can make the dog salivate and transform the bell and salivation into a conditioned stimulus and a conditioned response.

\subsection{Q-Learning. Sarsa}
\label{qlearning-sarsa}

\subsubsection{Q-Learning}
\label{qlearning}
Q-Learning\cite{norvig} is the algorithm for learning details about the environment in small steps. We know the initial starting state of the game and also a set of possible actions we can take in order to change the current state. At the beginning, we are constrained to choose randomly an action, because we do not know which one could lead us to win the game and this is the exploration phase. We keep randomly picking actions until the game is finished.

A game can have \textbf{n} number of states and \textbf{a} number of actions. We keep in memory a table \textbf{Q} of size \textbf{nxa} and we initialize all the corresponding elements with zero. During the game, rewards can be granted. Some of them can be good rewards and some of them can be bad rewards. When we receive rewards, we update the previous state with the obtained score.

Here we have to choose between exploration and exploitation. In the first states of the game, when we know nothing about the environment, we choose to explore new states in order to see if they bring us rewards. After a number of \textbf{e} episodes we learn which actions can bring us closer to the winning of the game. This is the moment when we start exploiting the things we have already learnt in the previous episodes. Assume we are in state \textbf{s} and have the next possible actions \{$a_{1},a_{2},a_{3},a_{4}$\}. Keep in mind that we recorded the Q-table. At this moment we choose the action which gives us the best utility from states s. If \textbf{Q(s,$a_{3}$)} is the maxim value of \textbf{Q} in state \textbf{s} we will pick action \textbf{$a_{3}$}.

\subsubsection{Sarsa}
\label{sarsa}
Q-Learning and Sarsa look alike except for the fact that Sarsa is updating its policy based on the action it takes which is not necessary the best action, but it is the action expected to pursue its policy.


When dealing with large negative rewards Q-Learning behaves better than Sarsa\cite{multi-agent} because Q-Learning does not avoid risk situations while Sarsa does. Both algorithms have variables that have to be adjusted during the computation.

\begin{itemize}
  \item \textbf{Learning rate} adjusts learning of new information flow. \textbf{0} is for not acquiring information and \textbf{1} is for considering only the new information.
  \item \textbf{Discount factor} adjust the importance of current rewards(\textbf{1}) vs future rewards(\textbf{0})
\end{itemize}

$\epsilon$-Greedy is the policy for choosing the next action. As we have already mentioned in the problem between choosing explorations vs. exploitation, this strategy is helpful when applied in the first few episodes when we don't have too many details about the environment. It says that we can choose randomly an action from the set of actions with probability $\epsilon$ and choose the best action with probability $1-\epsilon$.

Figure \ref{fig:svsq} shows behaviour of Q-Learning and Sarsa 

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=282px,height=288px]{src/img/state/qsrewards}
		\caption{Q-Learning vs Sarsa\cite{sutton}} \label{fig:svsq}
    \end{center}
\end{figure}


\section{Neural Networks and Convolutional Neural Networks}

In this chapter we will discuss how networks works, the principles behind them and what optimization techniques we can use. It is worth mentioning that the information is splitted in data, model, loss functions, train and test.
\begin{figure}[h]
	\begin{center}
		\includegraphics[width=294px,height=186px]{src/img/state/nn2}
		\caption{example of neural network} \label{fig:nn}
    \end{center}
\end{figure}

\subsection{Data}
\label{data}
The data resulted from the pre-processing step is crucial for learning. Differentiating between noisy images and/or bad lighting is a big problem for a network because it may learn the inappropriate features.

One of the first concerns is choosing the color space. We can have RGB(Red Green Blue) or we can have YUV(luminance blue–luminance red–luminance). In many cases RGB is the first choice instead of YUV\cite{pipeline}. If we are interested in color both of them are a good choice, but if we are interested in edges the luminance channel will be more suitable.

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=410px,height=102px]{src/img/state/lenayuv}
		\caption{Lena: RGB and Y, U, V channels} \label{fig:lenayuv}
    \end{center}
\end{figure}

If values have different ranges of scales, then logarithmic normalization should be used. In distance-based classification, if we have a feature with values between [0, 1] and another feature with values between [0, 2000] we need to normalize the data between [0, 1] or [-1, 1] such that every variation of each feature counts the same as the other features. We do not want to start by giving a higher level importance to one feature to the detriment the other ones.

Another aspect that should be considered is the importance of illumination conditions. If we are not interested in it then we remove the mean-value for each samples. If we have thousands of pictures with different illumination conditions we remove the average illumination by extracting the mean-value such that all the pictures have the same contrast.

Splitting data is another important aspect in training networks. We have to have three sets: train sets, test sets and validation sets. 50\% of initial data goes to training and the rest of fifteen percent is split in 25\% for testing and 25\% for validation. This is used when the dataset is very large. Having a small dataset implies the statetgy of splitting data into $\frac{2}{3}$ for training and $\frac{1}{3}$ for testing\cite{splitting}.


Cross validation\cite{cross-validation} should also be taken into consideration. Cross validation is the algorithm of splitting data into test dataset and train dataset forcing both sets cross-over in successive epochs such that every part in the initial dataset validates the model of the neural network. Assume that we have split data into \{\textbf{A}, \textbf{B}, \textbf{C}\}, each one being $\frac{1}{3}$ part of initial dataset. In the first iteration we take the model to train on \textbf{A} and \textbf{B} and test it with \textbf{C}. The second iteration implies the training on \textbf{A} and \textbf{C} and testing on \textbf{B} and the last iteration implies training on \textbf{B} and \textbf{C} and testing on \textbf{A}. In this way one can be assured that our model is working well for every case.



\subsection{Model}
\label{model}

%------------------------Nonlinear activation functions------------------------

\subsubsection{Nonlinear activation functions\cite{david,hiddenfunctions,backprop}}
\label{transfer}
Activation functions are used to define a connection between input and output.

\textbf{Hyperbolic tangent function(tanh)}
\begin{itemize}
	\item outputs values between 0 and 1
	\item outputs are zero-centred
	\item if extremely large negative numbers are given as input, the output will be close to -1 and the error will propagate correctly
	\item generally used in hidden layers
	\item tanh layers learn faster than sigmoid layers because of the size of the gradient
\end{itemize}

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=240px,height=180px]{src/img/state/tanh}
		\caption{tanh function} \label{fig:tanh}
    \end{center}
\end{figure}


\textbf{Sigmoid function(sigmoid)}
\begin{itemize}
  \item learning values farther away from the threshold is difficult
  \item useful for backpropagation algorithms
  \item easy for calculating derivatives and used in gradient descent methods
  \item output values between 0 and 1
  \item can be used in binary classification problems with setting some thresholds
  \item if extremely large negative numbers are given as input, the output will be closer to zero and the error will not propagate correctly
\end{itemize}

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=240px,height=180px]{src/img/state/sigmoid}
		\caption{sigmoid function} \label{fig:sigmoid}
    \end{center}
\end{figure}

\textbf{Rectified Linear Units(ReLU)}
\begin{itemize}
	\item better results in networks combined with stochastic gradient descent than tanh/sigmoid\cite{imagenet}
	\item ReLU is determined to be better than tanh/sigmoid empirically
\end{itemize}

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=240px,height=180px]{src/img/state/relu}
		\caption{ReLU} \label{fig:relu}
    \end{center}
\end{figure}

%------------------------Convolutional layers------------------------
\newpage
\subsubsection{Convolutional layers}
Pooling layers can be used when getting rid of unwanted information and also can be useful in dimensionality reduction.

\textbf{Spatial Convolution}
\begin{itemize}
	\item{used to modify the frequency of neighbourhood pixels}
	\item{accentuates edges in combination with Gabor filters}
	\item{can be used to obtain gaussian blur effect}
	\item{suppose we have an image of \textbf{width x height} size, if we apply a convolution kernel of size \textbf{kw x kh} the size of the output will become \textbf{(width - kw + 1) x (height - kh + 1)}}
\end{itemize}
~\\

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=240px,height=180px]{src/img/state/lena-spatialconv22}
		\caption{Lena: Spatial Convolution(2D) with 12 output channels, 5x5 kernel matrix and 2x2 stride} \label{fig:lena-spatialconv}
    \end{center}
\end{figure}

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=240px,height=180px]{src/img/state/gabor}
		\caption{Gabor filters\cite{gabor}} \label{fig:gabor.png}
    \end{center}
\end{figure}

\newpage
\textbf{Average pooling}
\begin{itemize}
	\item{useful for dimensionality reduction}
	\item{acts poorly in some situations as opposed to max-pooling}
\end{itemize}
~\\
\begin{figure}[h]
	\begin{center}
		\includegraphics[width=140px,height=140px]{src/img/state/lena-avgpooling}
		\caption{Lena: Average pooling with 10x10 kernel matrix and 2x2 stride} \label{fig:lena-avgpooling}
    \end{center}
\end{figure}

~\\

\textbf{Max pooling}
\begin{itemize}
	\item{useful for dimensionality reduction}
	\item{useful for activating features that have a low probability of being activated\cite{maxpooling}}
	\item{if a feature appears in a cell it is less probably to appear in the neighbourhood cells}
\end{itemize}
~\\
\begin{figure}[h]
	\begin{center}
		\includegraphics[width=140px,height=140px]{src/img/state/lena-maxpooling}
		\caption{Lena: Max pooling with 5x5 kernel matrix and 2x2 stride} \label{fig:lena-avgpooling}
    \end{center}
\end{figure}
\newpage
\textbf{LP pooling}

\begin{itemize}
	\item{the norm P=1 represents gaussian averaging and P=$\infty$ represents max pooling}
	\item{computes the information from neighbourhood if P>1}
\end{itemize}
~\\
\begin{figure}[h]
	\begin{center}
		\includegraphics[width=140px,height=140px]{src/img/state/lena-lp}
		\caption{Lena: LP pooling with pnorm = 10, 15x15 kernel matrix and 2.5x2.5 stride} \label{fig:lena-lppooling}
    \end{center}
\end{figure}


~\\




\textbf{Spatial Substractive Normalization}

\begin{itemize}
	\item{the norm P=1 represents gaussian averaging and P=$\infty$ represents max pooling}
	\item{computes the information from neighbourhood if P>1}
\end{itemize}
~\\
\begin{figure}[h]
	\begin{center}
		\includegraphics[width=140px,height=140px]{src/img/state/lena-spatialsubstractivenormalization}
		\caption{Lena: Spatial Substractive Normalization pooling with kernel size 11x1} \label{fig:lena-lppooling}
    \end{center}
\end{figure}
\newpage

\subsubsection{Feature visualization}
The technique called Deconvolutional Networks takes a neural network already trained and maps the features to pixels such that information from features learned can be human ``readable''.
~\\
\begin{figure}[h]
	\begin{center}
		\includegraphics[width=309px,height=173px]{src/img/state/features}
		\caption{Feature visualization\cite{fergus}} \label{fig:features}
    \end{center}
\end{figure}


\newpage
\subsection{Loss functions}
\label{loss}

Loss functions are used to approximate how far we are from the solution, the difference between observed values and predicted values.

\textbf{Mean squared error}
\begin{itemize}
	\item{useful for regression}
	\item{not used in classification problems}
\end{itemize}
\begin{equation}
J(weights) = \frac{1}{2\cdot no_{samples}}\cdot\displaystyle\sum_{i=1}^{no_{samples}}\cdot(nn_{weights}(samples^{(i)}) - labels^{(i)})^2
\end{equation}

\textbf{Hinge loss (margin criterion)}
\begin{itemize}
	\item{useful for binary classifications}
	\item{forces labels to be -1 or 1}
\end{itemize}
\begin{equation}
J(weights) = \displaystyle\sum_{i=1}^{no_{samples}}max(0, 1 - labels^{(i)}\cdot nn_{weights}(samples^{(i)}))
\end{equation}


\subsection{Optimizations for learning}
Optimization algorithms are used in updating the weights in a neural network in concordance with a loss function.
\label{optimization}
Assuming that our cost function is defined as follows\cite{ng}:
\begin{equation}
	J(weights_n) = \frac{1}{2no_{samples}}\cdot\displaystyle\sum_{i=1}^{no_{samples}}(nn_{weights}(samples^{(i)}) - labels^{(i)})^2
\end{equation}
we state that the derivative of \textbf{J} w.r.t \textbf{weights} is\cite{ng}:
\begin{equation}
	\frac{\partial J}{\partial weights} = \frac{1}{no_{samples}}\cdot\displaystyle\sum_{i=1}^{no_{samples}}(nn_{weights}(samples^{(i)}) - labels^{(i)})
\end{equation}
Updating weights\cite{ng} is done in accordance with the derivative of cost function.
\begin{equation}
	weights_n = weights_n - \alpha\cdot\frac{\partial J}{\partial weights}
\end{equation}
where $\alpha$ represents learning rate.

\newpage
\textbf{Gradient descent (GD)}

\begin{itemize}
	\item{if $\alpha$ is too small, GD converges slow}
	\item{if $\alpha$ is too large, GD can converge to local minimum and not global minimum}
	\item{recommended for huge number of samples}
	\item{require more iterations than SGD}
\end{itemize}

\begin{algorithm}
	\floatname{algorithm}{Algorithm}
	\caption{Gradient Descent\cite{ng}} \label{gd-code}
	\begin{algorithmic}[1]
		\Repeat
       			\For{\texttt{j = 1, no_weights}}
       				\State{\begin{equation}
					weights_j = weights_j - \alpha\displaystyle\sum_{i=1}^{no_{samples}}(nn_{weights}(samples^{(i)}) - labels^{(i)})\cdot {samples_j}^{(i)}
					\end{equation}}
      			\EndFor
		\Until{convergence is obtained}
	\end{algorithmic}
\end{algorithm}


\vspace{8mm}


\textbf{Stochastic gradient descent (SGD)}

\begin{itemize}
	\item{very fast is number of samples is small}
	\item{iterations are removed by computing $(X^T\cdot X)^{-1}$}
	\item{requires regularization}
	\item{needs feature scaling}
\end{itemize}

\begin{algorithm}
	\floatname{algorithm}{Algorithm}
	\caption{Stochastic Gradient Descent\cite{ng}} \label{sgd-code}
	\begin{algorithmic}[1]
		\Repeat
			\State shuffle dataset (randomly)
			\For{\texttt{i = 1, no_samples}}
       			\For{\texttt{j = 1, no_weights}}
       				\State\begin{equation}
					weights_j = weights_j - \alpha\cdot(nn_{weights}(samples^{(i)}) - labels^{(i)})\cdot {samples_j}^{(i)}
					\end{equation}
      			\EndFor
      		\EndFor
		\Until{convergence is obtained}
	\end{algorithmic}
\end{algorithm}

\newpage

\subsection{Training and testing}
\label{train-test}

The question which arises now is: when do we have to stop with training if a (convolutional) neural network ?
We have to take care of how much we train a network. If we train it too much, it will learn the samples and will predict exactly the same values, but there will be a large error for samples it has not seen. If we train it too little, the network will not learn how to predict the samples it has seen and will also have a large test error.
Mathematically, the moment we should stop with training is the moment when the test error will increase its value. In \ref{fig:traintest} we can see the optimum moment when we should stop training.

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=188px,height=129px]{src/img/state/traintest}
		\caption{train-test error\cite{traintest}} \label{fig:traintest}
    \end{center}
\end{figure}

Underfitting also means high bias and represents the incapacity of the algorithm to recognize the pattern features in samples.

Overfitting means also high variance and represents the inability for the algorithm to adapt to new samples.

Both of them are illustrated in the figure \ref{fig:overfitting}.

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=390px,height=147px]{src/img/state/overfitting}
		\caption{underfitting, bias-variance trade-off and overfitting\cite{ng}} \label{fig:overfitting}
    \end{center}
\end{figure}

\newpage

\section{An in-depth look to ``Human-level control through deep reinforcement learning'' paper}
This section is dedicated to analyzing the paper which is proposes a method for solving games using deep learning and reinforcement learning. The proposed agent, called Q-Network capable of playing 43 different games and adapting to each one without being modified.


\begin{figure}[h]
	\floatname{algorithm}{Algorithm}
	\begin{center}
		\includegraphics[width=334px,height=51px]{src/img/state/ataripic}
		\caption{Snapshots with five Atari 2600 Games: Pong, Breakout, Space Invaders, Seaquest, Beam Rider\cite{nature}} \label{fig:ataripic}
    \end{center}
\end{figure}




\subsection{Architecture}
The article\cite{nature} proposes a new type of network, Q-network which combines convolutional layers, a representation of the human receptive fields and pooling layers used for dimensionality reduction. The input layer is represented by \textbf{84x84x4} neurons for the image produced after preprocessing frames from the game and is followed by three convolutional layers(32 filters of 8x8 and stride 4 with ReLU, 64 filters of 4x4 and stride 2 with ReLU, 64 filters of 3x3 and stride 1 with ReLU), two fully connected layers: a hidden layer with 512 units and the output layer which has a number of neurons equal to the number of actions corresponding to each game. Each hidden layer is followed by a rectified linear unit (ReLU).



\begin{figure}[h]
	\floatname{algorithm}{Algorithm}
	\begin{center}
		\includegraphics[width=442px,height=242px]{src/img/state/schematic-ilustration}
		\caption{Q-Network architecture\cite{nature}} \label{fig:arch}
    \end{center}
\end{figure}

The network is used to approximate the optimal action-value function $Q^{*}$\cite{nature}:
\begin{equation}
	Q^{*}(s,a) = \max_\pi E[r_t + \gamma\cdot r_{t+1} + \gamma^2\cdot r_{t+2} + \dotsc | s_t = s, a_t = a, \pi]
\end{equation}
The function Q is also parametrized with the weights and the agent state $e_t = (s_t,a_t,r_t,s_{t+1})$ is saved at each iteration in a set D_t = ${(e_1, e_2, \dotsc, e_t)}$
With all the necessary information here is the computation of the loss function\cite{nature}:

\begin{equation}
	L_i(\theta_i)=E_{(s,a,r,s^{\prime})}[(r + \gamma\cdot\max_{a^{\prime}}\cdot Q(s^{\prime},a^{\prime};{\theta_i}^-) - Q(s,a;\theta_i))^2]
\end{equation}

where $\gamma$ represents the discount factor and ${\theta_i}^-)$ is updated with $\theta_i$ every x defined steps and here is the gradient\cite{nature}:

\begin{equation}
\nabla_{\theta_i}L(\theta_i) = E_{s,a,r,{s^i}}[(r + \gamma\cdot\max_{a^{\prime}}Q(s^\prime, a^\prime;{\theta_i}^-)-Q(s,a;\theta_i))\nabla_{\theta_i}Q(s,a;\theta_i)]
\end{equation}

The Q-learning action-value function can be updates using ${\theta_i}^-$ = $\theta_{i-1}$ following the optimal policy with probability 1 - $\epsilon$ and selecting a random move with probability $\epsilon$. 

For minimizing the loss function, Stochastic Gradient Descent is used in combination with the experience replay technique for preventing `dead' neurons and also, Q is not updated at each iteration. This avoids the instability of Q caused by the nonlinearity of network.

The $\epsilon$-greedy policy was set to 0.05 and the Q-values were scaled because of big range values from rewards.
\newpage
\subsection{Algorithm}
The algorithm\cite{nature} used for training Q-Network is presented as it follows:
\begin{algorithm}
	\floatname{algorithm}{Algorithm}
	\caption{Q-Network} \label{sgd-code}
	\begin{algorithmic}[1]
		\State create replay memory D for storing N experiences
		\State choose $\epsilon$ between (0,1)
		\State init Q model weights with $\theta$
		\State save $Q^-$ model weights to $\theta^-$
		\State init variable episode to 0 and choose MAX_EPISODES
		\While{episode < MAX_EPISODES}{
			\State generate state $s_1$ from frame: $s_1$ = {$image_1$}
			\While{game not over}
				\State r = random number between (0,1)
				\If{r < $\epsilon$}
					\State $a_t$ = random action
				\Else
					\State $a_t$ = $argmax_a$ Q($s_t$, a; $\theta$)
				\EndIf

				\State $s_{t+1}$ = ($s_t$,$a_t$,$image_{t+1}$)

				\State D = D + \{($s_t$, $a_t$, $r_t$, $s_{t+1}$)\}

				\State shuffle D

				\State extract ($s_j$, $a_j$, $r_j$, $s_{j+1}$) from D

				\If{episode is finished at step j+1}
					\State $y_j$ = $r_j$
				\Else
					\State $y_j$ = $r_j$ + $\gamma\cdot\max_{a^{\prime}}\cdot {Q^-}(s_{j+1},a^{\prime};\theta^-)$
				\EndIf
				\State propagate error $(y_j - Q(s_j,a_j;\theta))^2$
				\State at each \textbf{x} steps save model weights $\theta$ to $\theta^-$
			\EndWhile
		}\EndWhile

		
	\end{algorithmic}
\end{algorithm}

\newpage


\subsection{Results}

Q-Network is capable of learning the optimal policy. For example, in Breakout what is being learned is to make a tunnel between blocks in order to send the ball into the back. Receiving only the pixel from the frames, rewards and using the same network structure it is capable of playing different games. The Q-Network can achieve 75\% of a score of a human test at 49 games\cite{nature}.


\begin{figure}[h]
	\floatname{algorithm}{Algorithm}
	\begin{center}
		\includegraphics[width=367px,height=435px]{src/img/state/comparision}
		\caption{Comparision\cite{nature}} \label{fig:comp}
    \end{center}
\end{figure}




















